{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533203a6",
   "metadata": {},
   "source": [
    "# GEOPHYS 257 (Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)\n",
    "\n",
    "## Multicore Parallelization of Matrix Multiplication Using Numba\n",
    "\n",
    "In this lab we will be using Numba to accelerate matrix-matrix multiplications by exploiting parallelism. Even most laptops today have Multicore CPUs, where a *core* is a microprocessor, and each core is usually a copy of the each other core. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba](https://numba.readthedocs.io/en/stable/index.html): Documentation\n",
    "* [Numba in 30 min](https://youtu.be/DPyPmoeUdcE): Conference presentation video\n",
    "\n",
    "\n",
    "## Required Preperation\n",
    "Please watch the following videos before starting the lab (each is pretty short):\n",
    "* [Introduction to Parallel Computing](https://youtu.be/RNVIcm8-6RE)\n",
    "* [Amdahl's Law](https://youtu.be/Axx2xuB-Xuo)\n",
    "* [CPU Caching](https://youtu.be/KmairurdiaY)\n",
    "* [Pipelining](https://youtu.be/zPmfprtdzCE)\n",
    "* [Instruction Level Parallelism](https://youtu.be/ZoDUI3gTkDI)\n",
    "* [Introduction to SIMD](https://youtu.be/o_n4AKwdfiA)\n",
    "\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "Please run the following cells.  Examine the result of the example function that makes use of the *timer* wrapper. Also please use this timer wrapper (defined below) for all the exercises that follow.\n",
    "\n",
    "#### Load python modules (Note: you may need to install some Python packages for the modules below, e.g. Numba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9957106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from time import time, sleep\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc2aac",
   "metadata": {},
   "source": [
    "#### Define a function-decorator for timing the runtime of your functions\n",
    "\n",
    "Below is some code that you can use to wrap your functions so that you can time them individually.  The function defined imeadiately below the *timer()* is an example of how to use the warpper. In the cell that follows, execute this example function. (Note, for this timer, were are making use of something called *decorators*, but a discussion about this feature is outside the scope of this class.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd0a801",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# defining a function decorator for timing other functions\n",
    "def mytimer(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        t_start = time() # Students look here\n",
    "        result = func(*args, **kwargs)\n",
    "        t_end = time() # Students also look here. This is how you can time things inside functions/classes\n",
    "        print(f'Function: \\'{func.__name__}({kwargs})\\' executed in {(t_end-t_start):4.3f}s\\n')\n",
    "        return result\n",
    "    return wrap\n",
    "    \n",
    "\n",
    "# Example of how to use. NOTE the \"@mytimer\" stated just above the function definition\n",
    "@mytimer\n",
    "def example_sum_timer_wrap(N):\n",
    "    \"\"\" Sum the squares of the intergers, i, contained in [1-N] \"\"\"\n",
    "    return np.sum(np.arange(1,N+1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb7b5c",
   "metadata": {},
   "source": [
    "#### Run the example function\n",
    "\n",
    "Run the example function for each of the following instances of $N = 10^5, 10^6, 10^7, 10^8$. Please examine the results, in particular, how the runtime changes with respect to $N$.\n",
    "\n",
    "\n",
    "**Answer** the following questions in the markdown cell that follow the code. \n",
    "* For each factor-of-ten increase in $N$, roughly how much longer was the runtime of the function?\n",
    "* Does this slowdown in the runtime make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa466992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Call the example function above for each value of N (try making one-call first, then loop)\n",
    "N=[1e5,1e6,1e7,1e8]\n",
    "print(len(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d4edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'example_sum_timer_wrap({})' executed in 0.001s\n",
      "\n",
      "<function mytimer.<locals>.wrap at 0x7f8b3eb4aa70>\n",
      "Function: 'example_sum_timer_wrap({})' executed in 0.006s\n",
      "\n",
      "<function mytimer.<locals>.wrap at 0x7f8b3ebba680>\n",
      "Function: 'example_sum_timer_wrap({})' executed in 0.055s\n",
      "\n",
      "<function mytimer.<locals>.wrap at 0x7f8b3ebba680>\n",
      "Function: 'example_sum_timer_wrap({})' executed in 0.611s\n",
      "\n",
      "<function mytimer.<locals>.wrap at 0x7f8b3eb4aa70>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(N)):\n",
    "    print(mytimer(example_sum_timer_wrap(N[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4c88",
   "metadata": {},
   "source": [
    "#### Your answer for Exerciese 0 questions here:\n",
    "\n",
    "For each factor of ten increase in N, there's approximately a factor of 10 increase in runtime. This slowdown in runtime makes sense because the run time should scale ~linearly with the number of operations it should perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8161fc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 1: Naive matrix multiplication: \n",
    "\n",
    "For this exercise, we will write our own matrx-matrix multiplication function.  We are goint to be naieve about it, so we want to loop over individual indices, instead of using slicing (in the next section, we will parallelize this code and we want to see how well Numba does at speeding up the naive code).\n",
    "\n",
    "#### Tasks for this exercise\n",
    "\n",
    "1. Write a function with that calculates matrix-matrix multiplication such that $C = A\\cdot B$, where $A$, $B$, and $C$ are 2D numpy arrays. Make the dtype for the $C$ matrix the same as the dtype for $A$. Below is a stencil you can start with. test your results for accuracy and performance against the np.dot() function. To time the np.dot() function, you can wrap it in another function and use the *mytimer* wrapper; however, please opy the code for testing the A and B dimensions into your function wrapper for the np.dot() function.  This keeps the runtime comparison as a more \"apples-to-apples\" like comparison.\n",
    "\n",
    "```python\n",
    "@mytimer\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in range(<val>):\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Define your functions in the cell below.\n",
    "3. Test and compare the accuracy and runtime of your functions in the next cell.\n",
    "    - Test with non-square Matrices: $A \\in \\mathbb{R}^{N\\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$. With $N = 64$, $K = 32$, and $M = 128$.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 64, 128,$ and $256$.\n",
    "    - Test the following three cases:\n",
    "        - Case-1. $A$ and $B$ **both** have dtype=np.**float32** (make sure that $C$ is also of dtype=np.**float32**)\n",
    "        - Case-2. $A$ has dtype=np.**float64**, but $B$ has dtype=np.**float32**\n",
    "        - Case-3. $A$ and $B$ **both** have dtype=np.**float64** (make sure that $C$ is also of dtype=np.**float64**)\n",
    "    - For all three case above:\n",
    "        - Calculate and show the error by computing the sum of the differnece between the $C$ matrices computed from numpy.dot() and your my_naive_matmul() functions. Assume that numpy.dot() is correct\n",
    "        - Calculate and show the *speedup* that the faster function has versus the slower function.\n",
    "        - Comment on the which of the three cases is fastest, and comment on what the speedup of the fastest case is and why it is the fastest case.\n",
    "4. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "A = np.random.rand(N, K)\n",
    "B = np.random.rand(\"for-you-to-figure-out\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 1 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6404b75",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 1 here.\n",
    "@mytimer\n",
    "def mynaivematmul(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for row in range(rows): \n",
    "        for col in range(cols):\n",
    "            #for elt in range(B.shape[0]):\n",
    "            for elt in range(A.shape[1]):\n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3962ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mytimer\n",
    "def npcomparison(A,B):\n",
    "    C=np.dot(A,B)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232378",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 1 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea011152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Function: 'mynaivematmul({})' executed in 0.147s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "3.637978807091713e-12\n",
      "(64, 32)\n",
      "64\n",
      "Function: 'mynaivematmul({})' executed in 0.148s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "(64, 64)\n",
      "128\n",
      "Function: 'mynaivematmul({})' executed in 1.169s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "(128, 128)\n",
      "256\n",
      "Function: 'mynaivematmul({})' executed in 9.423s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.001s\n",
      "\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Section Goal: Test your code here for runtime and accuracy\n",
    "# Task Goal: Test with A=64x32 matrix, B=32x128 matrix\n",
    "\n",
    "Aa = np.random.rand(64,128)\n",
    "Ba = np.random.rand(128,32)\n",
    "\n",
    "print(Ba.shape[1])\n",
    "\n",
    "Ca = mynaivematmul(Aa,Ba)\n",
    "Cacomp=npcomparison(Aa,Ba)\n",
    "\n",
    "print(np.sum(np.abs(Ca-Cacomp)))\n",
    "print(Ca.shape)\n",
    "\n",
    "# Task Goal: Test with square matrices: N=64, 128, 256\n",
    "#N=64\n",
    "Ab = np.random.rand(64,64)\n",
    "Bb = np.random.rand(64,64)\n",
    "\n",
    "print(Bb.shape[1])\n",
    "\n",
    "Cb = mynaivematmul(Ab,Bb)\n",
    "Cbcomp=npcomparison(Ab,Bb)\n",
    "\n",
    "print(Cb.shape)\n",
    "#N=128\n",
    "\n",
    "Ac = np.random.rand(128,128)\n",
    "Bc = np.random.rand(128,128)\n",
    "\n",
    "print(Bc.shape[1])\n",
    "\n",
    "Cc = mynaivematmul(Ac,Bc)\n",
    "Cccomp=npcomparison(Ac,Bc)\n",
    "\n",
    "print(Cc.shape)\n",
    "#N=256\n",
    "Ad = np.random.rand(256,256)\n",
    "Bd = np.random.rand(256,256)\n",
    "\n",
    "print(Bd.shape[1])\n",
    "\n",
    "Cd = mynaivematmul(Ad,Bd)\n",
    "Cdcomp=npcomparison(Ad,Bd)\n",
    "\n",
    "print(Cd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d678f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'mynaivematmul({})' executed in 0.146s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "0.0019168854\n",
      "float32\n",
      "Function: 'mynaivematmul({})' executed in 0.166s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "3.396394276933279e-12\n",
      "float64\n",
      "Function: 'mynaivematmul({})' executed in 0.146s\n",
      "\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "3.4798830483850907e-12\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "#Task Goal: Testing 3 Cases of different dtypes\n",
    "A1=np.random.rand(64,64).astype('float32')\n",
    "B1=np.random.rand(64,64).astype('float32')\n",
    "C1 = mynaivematmul(A1,B1)\n",
    "C1comp=npcomparison(A1,B1)\n",
    "print(np.sum(np.abs(C1-C1comp)))\n",
    "print(C1.dtype)\n",
    "\n",
    "A2=np.random.rand(64,64).astype('float64')\n",
    "B2=np.random.rand(64,64).astype('float32')\n",
    "C2 = mynaivematmul(A2,B2)\n",
    "C2comp=npcomparison(A2,B2)\n",
    "print(np.sum(np.abs(C2-C2comp)))\n",
    "print(C2.dtype)\n",
    "\n",
    "A3=np.random.rand(64,64).astype('float64')\n",
    "B3=np.random.rand(64,64).astype('float64')\n",
    "C3 = mynaivematmul(A3,B3)\n",
    "C3comp=npcomparison(A3,B3)\n",
    "print(np.sum(np.abs(C3-C3comp)))\n",
    "print(C3.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31543b82",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Exercise 1 Responses:\n",
    "- Comment on the which of the three cases is fastest, and comment on what the speedup of the fastest case is and why it is the fastest case.\n",
    "\n",
    "Of the three cases tested, the fastest case was the case where the A and B matrices are both float32. This is because they are the same dtype and carry less information, thus are able to carry out calculations faster. The slowest case was the case where A and B were different floats. This slowdown is probably due to typecasting, where the B matrix had to be typecast to float64 to match the dtype of the A matrix. Typecasting adds time because it's essentially another operation.\n",
    "\n",
    "Of the three cases, the most accurate case was the case where the A and B matrices are both float64. This is because they have the highest precision floats of the cases and aren't prone to typecasting errors due to the initial standardization of matrix dtypes. The least accurate case was where the A and B matrices are both float32. This is because they float32 is less precise than its float64 counterpart, meaning this case is more prone to roundoff errors.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae756d65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 2: Using Numba to speed up matrix multiplication: \n",
    "\n",
    "For this exercise, numba to speedup our matrx-matrix multiplication function below. However, there is an interesting twist to this exercise. We will write six versions of the naive function you wrote above. One function for each of the six possible permutations of three loops used to calculate the multplication (i.e. ijk, ikj, jik, etc.)\n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Make six copies your code for the my_naive_matmul(), one copy for each of the possible permutations and define them in the cell bellow:\n",
    "    - One of these function will have the same loop order as the my_naive_matmul() function. \n",
    "    - However name these functions: numba_mul_\\<perm\\>(), where \\<perm\\> should be replaced by the specific loop order of the function.\n",
    "2. In the cell that follows your function definitions, test and compare the accuracy and runtime of your functions against the numpy.dot() function.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 128, 256,$ and $512$.\n",
    "    - For each matrix set their dtype as: dtype=np.float64\n",
    "    - Calculate and show the error between your functions and the numpy.dot() function. (Same as in Exercise 1.)\n",
    "    - Calculate and show the *speedup* that the fastest function has versus the all the other functions.\n",
    "    - You should notice that one of your permutation functions is faster than the others. For this case show the following:\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has versus the my_naive_matmul().\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has when $A$, $B$, and $C$ are all of dtype=np.float64 vs all are of dtype=np.float32.\n",
    "3. Create your matrices using random numbers. (Same as in Exercise 1.) \n",
    "4. For each function, you need to add a function decorator for numba. Numba will *JIT* the function (**J**ust **I**n **T**ime compilation). \n",
    "    - Use the flag to keep a \"cache\" of the compiled code (not to be confused with CPU cache). **Note**: to get accurate timings, you will need to run your tests **twice**, because during the first run, the code is compiled and this compile time will be included in your runtime. After the first run, the compiled binary will be stored, so consecutive runs will be faster.\n",
    "    - Use the flag to use *fast math*\n",
    "    - Use the flag to disable the Python Global Interpretor Lock (GIL).\n",
    "    - The code below should get you started, but it is incomplete.\n",
    "    \n",
    "```python   \n",
    "    \n",
    "@mytimer\n",
    "@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "def numba_mul_<perm>(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in numba.prange(<val>): # !!!! LOOK HERE !!!!\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "    \n",
    "6. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the questions asked in the markdown cell.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dc104d3",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 2 here.\n",
    "\n",
    "#original:i,j,k\n",
    "@mytimer\n",
    "def numba_mul_ijk(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for row in range(rows): \n",
    "        for col in range(cols):\n",
    "            #for elt in range(B.shape[0]):\n",
    "            for elt in range(A.shape[1]):\n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "\n",
    "#variant: i,k,j\n",
    "@mytimer\n",
    "def numba_mul_ikj(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for row in range(rows): \n",
    "        for elt in range(A.shape[1]):\n",
    "            for col in range(cols):\n",
    "                \n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "\n",
    "#variant: j,i,k\n",
    "@mytimer\n",
    "def numba_mul_jik(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for col in range(cols):\n",
    "        for row in range(rows): \n",
    "            #for elt in range(B.shape[0]):\n",
    "            for elt in range(A.shape[1]):\n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "\n",
    "#variant: jki\n",
    "@mytimer\n",
    "def numba_mul_jki(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for col in range(cols):\n",
    "        for elt in range(A.shape[1]):\n",
    "            for row in range(rows): \n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "\n",
    "#variant: kij\n",
    "@mytimer\n",
    "def numba_mul_kij(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for elt in range(A.shape[1]):\n",
    "        for row in range(rows): \n",
    "            for col in range(cols):\n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "\n",
    "\n",
    "#variant: kji\n",
    "@mytimer\n",
    "def numba_mul_kji(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for elt in range(A.shape[1]):\n",
    "        for col in range(cols):\n",
    "            for row in range(rows): \n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72d6775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mytimer\n",
    "def npcomparison(A,B):\n",
    "    C=np.dot(A,B)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "264e6dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'npcomparison({})' executed in 0.001s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 1.160s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 1.158s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 1.136s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 1.183s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 1.170s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 1.181s\n",
      "\n",
      "2.7640112421067897e-11\n",
      "2.7640112421067897e-11\n",
      "2.7640112421067897e-11\n",
      "2.7640112421067897e-11\n",
      "2.7640112421067897e-11\n",
      "2.7640112421067897e-11\n",
      "Function: 'npcomparison({})' executed in 0.001s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 9.144s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 9.295s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 9.117s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 9.386s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 9.301s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 9.464s\n",
      "\n",
      "1.4426149164137314e-09\n",
      "1.4426149164137314e-09\n",
      "1.4426149164137314e-09\n",
      "1.4426149164137314e-09\n",
      "1.4426149164137314e-09\n",
      "1.4426149164137314e-09\n",
      "Function: 'npcomparison({})' executed in 0.002s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 75.726s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 74.801s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 76.763s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 78.808s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 77.654s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 78.235s\n",
      "\n",
      "1.582148456691357e-08\n",
      "1.582148456691357e-08\n",
      "1.582148456691357e-08\n",
      "1.582148456691357e-08\n",
      "1.582148456691357e-08\n",
      "1.582148456691357e-08\n"
     ]
    }
   ],
   "source": [
    "N=[128,256,512]\n",
    "i=0\n",
    "while i<len(N):\n",
    "    A=np.random.rand(N[i],N[i]).astype('float64')\n",
    "    B=np.random.rand(N[i],N[i]).astype('float64')\n",
    "    Ccomp=npcomparison(A,B)\n",
    "    \n",
    "    Cijk = numba_mul_ijk(A,B)\n",
    "    Cikj = numba_mul_ikj(A,B)\n",
    "    Cjik = numba_mul_jik(A,B)\n",
    "    Cjki = numba_mul_jki(A,B)\n",
    "    Ckij = numba_mul_kij(A,B)\n",
    "    Ckji = numba_mul_kji(A,B)\n",
    "\n",
    "    print(np.sum(np.abs(Cijk-Ccomp)))\n",
    "    print(np.sum(np.abs(Cikj-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjik-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjki-Ccomp)))\n",
    "    print(np.sum(np.abs(Ckij-Ccomp)))\n",
    "    print(np.sum(np.abs(Ckji-Ccomp)))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a808c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'npcomparison({})' executed in 0.001s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 1.141s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 1.161s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 1.144s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 1.171s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 1.166s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 1.175s\n",
      "\n",
      "0.015016556\n",
      "0.015016556\n",
      "0.015016556\n",
      "0.015016556\n",
      "0.015016556\n",
      "0.015016556\n",
      "Function: 'npcomparison({})' executed in 0.000s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 9.108s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 9.307s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 9.088s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 9.357s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 9.368s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 9.429s\n",
      "\n",
      "0.11828232\n",
      "0.11828232\n",
      "0.11828232\n",
      "0.11828232\n",
      "0.11828232\n",
      "0.11828232\n",
      "Function: 'npcomparison({})' executed in 0.014s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 76.850s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 74.741s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 76.555s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 78.970s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 77.940s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 79.265s\n",
      "\n",
      "8.626694\n",
      "8.626694\n",
      "8.626694\n",
      "8.626694\n",
      "8.626694\n",
      "8.626694\n"
     ]
    }
   ],
   "source": [
    "N=[128,256,512]\n",
    "i=0\n",
    "while i<len(N):\n",
    "    A=np.random.rand(N[i],N[i]).astype('float32')\n",
    "    B=np.random.rand(N[i],N[i]).astype('float32')\n",
    "    Ccomp=npcomparison(A,B)\n",
    "    \n",
    "    Cijk = numba_mul_ijk(A,B)\n",
    "    Cikj = numba_mul_ikj(A,B)\n",
    "    Cjik = numba_mul_jik(A,B)\n",
    "    Cjki = numba_mul_jki(A,B)\n",
    "    Ckij = numba_mul_kij(A,B)\n",
    "    Ckji = numba_mul_kji(A,B)\n",
    "\n",
    "    print(np.sum(np.abs(Cijk-Ccomp)))\n",
    "    print(np.sum(np.abs(Cikj-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjik-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjki-Ccomp)))\n",
    "    print(np.sum(np.abs(Ckij-Ccomp)))\n",
    "    print(np.sum(np.abs(Ckji-Ccomp)))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9df8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing numba\n",
    "'''\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=False)\n",
    "#@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "#@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "def numba_mul_ijk(A,B):\n",
    "  global C\n",
    "  if  A.shape[1] == B.shape[0]:\n",
    "    Ctype=A.dtype\n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype = Ctype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    for row in numba.prange(rows): \n",
    "        for col in numba.prange(cols):\n",
    "            #for elt in range(B.shape[0]):\n",
    "            for elt in numba.prange(A.shape[1]):\n",
    "              C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "  else:\n",
    "    return \"Can't multiply matrices\"\n",
    "    \n",
    "    #Didn't work because njit has trouble reading Python. Also, be careful for the if statements. Parallelization \n",
    "    #makes multiple branches. This means that the program gets confused on waiting and chooseing what to execute.\n",
    "    \n",
    "'''\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_ijk(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for row in numba.prange(rows): \n",
    "        for col in range(cols):\n",
    "            for elt in range(A.shape[1]):\n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e871a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_ikj(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for row in numba.prange(rows): \n",
    "        for elt in range(A.shape[1]):\n",
    "            for col in range(cols):\n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_jik(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for col in numba.prange(cols):\n",
    "        for row in range(rows): \n",
    "            for elt in range(A.shape[1]):\n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_jki(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for col in numba.prange(cols):\n",
    "        for elt in range(A.shape[1]):\n",
    "            for row in range(rows): \n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_kij(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for elt in numba.prange(A.shape[1]):\n",
    "        for row in range(rows): \n",
    "            for col in range(cols):\n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def numba_mul_kji(A,B):\n",
    "    \n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception('A and B have incompatible dims')\n",
    "        \n",
    "    C = np.zeros((A.shape[0],B.shape[1]),dtype=A.dtype)\n",
    "    rows=A.shape[0]\n",
    "    cols=B.shape[1]\n",
    "    \n",
    "    for elt in numba.prange(A.shape[1]):\n",
    "        for col in range(cols):\n",
    "            for row in range(rows): \n",
    "                C[row, col] += A[row, elt] * B[elt, col]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee94d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'npcomparison({})' executed in 0.008s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 0.014s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 1.483s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 1.383s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 1.322s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 1.371s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 1.451s\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1632.0574\n",
      "14362.732\n",
      "Function: 'npcomparison({})' executed in 0.008s\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 0.024s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 0.030s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 0.026s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 0.040s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 0.004s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 0.042s\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "608.27234\n",
      "31695.443\n"
     ]
    }
   ],
   "source": [
    "N=[128,256]\n",
    "i=0\n",
    "while i<len(N):\n",
    "    A=np.random.rand(N[i],N[i]).astype('float32')\n",
    "    B=np.random.rand(N[i],N[i]).astype('float32')\n",
    "    Ccomp=npcomparison(A,B)\n",
    "    \n",
    "    Cijk = numba_mul_ijk(A,B)\n",
    "    Cikj = numba_mul_ikj(A,B)\n",
    "    Cjik = numba_mul_jik(A,B)\n",
    "    Cjki = numba_mul_jki(A,B)\n",
    "    Ckij = numba_mul_kij(A,B)\n",
    "    Ckji = numba_mul_kji(A,B)\n",
    "    \n",
    "    print(np.sum(np.abs(Cijk-Ccomp)))\n",
    "    print(np.sum(np.abs(Cikj-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjik-Ccomp)))\n",
    "    print(np.sum(np.abs(Cjki-Ccomp)))\n",
    "\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327e73a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 2 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code here for runtimes, accuracy, and speedups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c61afc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Discussion for Exercise 2\n",
    "1. What do you think is causing the differences in performance between the various permutations?\n",
    "1. Which function is fastest (permutations or numpy.dot)? Why do you think this function is the fastest, and could there be multiple factors involved regarding the superior performance?\n",
    "1. When comparing the matrix-matrix performance between the cases were all matrices had dtype=np.float64 vs.  all matrices having dtype=np.float32, which dtype was fastest? Roughtly what was the speedup when using this dtype vs the other?\n",
    "1. Does Amdahl's Law play a major factor in the performance differences? Which part of the matrix-matrix multiplication was not parallelized (serial portion)? (Hint: which matricies did we reuse for each function?) Could we parallelize this part, and if so, are there caveats?\n",
    "1. Do you think all codes should be parallelized? What about matrix-matrix multiplication? (This is a subjective question, but I am looking for a brief, but rational and informed arguement).\n",
    "1. For those taking the class for **4 units**: Regarding the performance differences, which Law do you think is more relevant when comparing the performance differences between each of the functions in this exercise, Amdahl's Law or Gustafson's Law? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea7a26",
   "metadata": {},
   "source": [
    "#1 I think that the differences is caused by cache efficiency. It is best to take the direction where the fastest caches are being utilized. This direction might be dependent on the program that you use (ex. Python vs C).\n",
    "\n",
    "#2 The function that performed the fastest was the numpy.dot operation. This is because it runs off of already efficient C code. The parralelization-compatible wraparounds caused by the permutations will always have slightly more operations than this numpy.dot function, meaning the run time must always be longer for the permutaions.\n",
    "\n",
    "#3 Operations with the float32 dtype were approximately twice as fast as their float64 counterparts. The reason for this is similar to the explanation described in Exercise 1.\n",
    "\n",
    "#4 Amdahl's Law states \"the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used\". I think that this law plays an important factor in the performance differences (parallel vs not), because it increased the efficiency of the algebraic operations involved in the matrix-matrix multiplication process. By parallelizing the algebraic processes, the program divides the workload of the slowest part of the code, increasing the overall speed. In the code, the matrix vectors (1D row or column) are not parallelized. It would be possible to parallelize this part; however, this would take a lot of effort and time. Not worth yo.\n",
    "\n",
    "#5 I don't think that all codes should be parallelized. It takes time for the original code to be rewritten ST it is compatible with Numba. This might be more of a slowdown for the programmer, especially if the code isn't computationally expensive. I do however believe that most matrix-matrix multiplication codes should be parallelized. This is because you get a dramatic reduction in run-time. Chances are that if you need to do matrix multiplications, you'll to perform various operations. This is more than enough just cause to warrant parallelizaiton.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef5ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
