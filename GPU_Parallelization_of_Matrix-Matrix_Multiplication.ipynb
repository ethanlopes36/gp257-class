{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b83b7a",
   "metadata": {},
   "source": [
    "\n",
    "# GPU Numba and CuPy Parallelization of Matrix Multiplication \n",
    "\n",
    "Similary to the multicore parallelization lab, in this lab we will be using Numba and CuPy to accelerate matrix-matrix multiplications using GPU. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba for CUDA](https://numba.readthedocs.io/en/stable/cuda/index.html)\n",
    "* [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb)\n",
    "* [Numba.CUDA by Graham Markell](https://github.com/numba/nvidia-cuda-tutorial)\n",
    "* [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/)\n",
    "* [CuPy Basics](https://docs.cupy.dev/en/stable/user_guide/basic.html)\n",
    "\n",
    "[//]: <> (GEOPHYS 257 Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff49d1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "* You need to request a *T4* node on the cluster. Don't forget that you need to add **--gres=gpu** to your srun command.\n",
    "* Reminder: on the *T4* nodes you need to load a different version of Python:\n",
    "```bash\n",
    "spack load python@3.10.7\n",
    "```\n",
    "\n",
    "* Import every Python module, object, and/or function that you need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e913f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e9a09",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Matrix Transpose\n",
    "\n",
    "Before we examine matrix-matrix multiplication, we will first write a GPU kernel that transposes a square matrix.  This type of problem is a good introduction into how to use the CUDA threading model. The task for this exercise is to write a Numba CUDA kernel that will transpose a square matrix. \n",
    "\n",
    "**Before you start**, take a look at the following:\n",
    "* Read over the following notebook that explanes Numba.CUDA kernels: [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb) \n",
    "* The first matrix-matrix multiplication code (the one that **doesn't** use shared memory) shown at [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). Understanding this code should give a pretty good idea on how to write the transpose kernel. The matrix-matrix kernel code from the NYU lab is shown below.\n",
    "```python\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "```\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Write a Numba.CUDA kernel that transpose an $NxN$ square matrix.\n",
    "* Be sure that the transpose kernel can transpose square matrices with sizes of $N$ as small as $N=2$ and as large as $N=10240$.\n",
    "* Using shared memory is **not** required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e121b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_kernel(A, B):\n",
    "    i, j = cuda.grid(2)\n",
    "    N = A.shape[0]\n",
    "    if i < N and j < N:\n",
    "        B[i, j] = A[j, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83310868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Input Matrix\n",
    "N = 1024\n",
    "A = np.random.rand(N, N)\n",
    "\n",
    "# Output will be the same size as the input matrix\n",
    "B = np.zeros((N, N))\n",
    "\n",
    "# Define the block size and grid size\n",
    "block_size = (32, 32)\n",
    "grid_size = (int(np.ceil(N/block_size[0])), int(np.ceil(N/block_size[1])))\n",
    "\n",
    "# Kernel\n",
    "transpose_kernel[grid_size, block_size](A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12070622",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Using Numba CUDA to parallelize matrix multiplication: \n",
    "\n",
    "For this exercise, we will use Numba compiled GPU kernels that calculate matrix-matrix multiplication for square matrices. In particular, we will use a GPU kernel that doesn't used shared memory and compared to a GPU kernel that does use shared-memory. Please use the two kernel codes discussed in the following lab: [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). As you will see in this exercise, learning to use shared-memory (akin to user-controlled cache), can take a lot of practice, so in the next exercise, we examine how well the simple shared-memory kernel from the NYU lab compares to the optimized codes provided by NVIDIA in the CuPy package. \n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Copy the matrix-matrix kernel codes from the NYU lab. Test them for accurracy against *numpy.dot()* and also compare time runtimes these GPU kernels the numpy.dot() function as well. **Note:** Use [CUDA events](https://numba.readthedocs.io/en/stable/cuda-reference/host.html#events) when timing GPU kernel calls because the driver does not \"block\" the calling process (for case this is IPython). Insted, the kernel is sent to the GPU to run, and then the process (IPython) immediately continues to it's next bit of code. Contrary to GPU kernel calls, calls to copy data to or from the GPU will block the process. For these cases, the calls can be timed the same way that other Python calls are timed.<br> **For both GPU kernels:**\n",
    "    - Test with square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 5120$, $N=10240$, and $N=20480$. **Tip**, first make sure you can get the GPU codes to work and that you get correct results by testing with $N_{test}=32$.\n",
    "    - For each $N$ above, test the multiplication for both dtypes: *dtype=float32* and *dtype=float64*.\n",
    "    - Calculate and show the error between your functions and the *numpy.dot()* function. \n",
    "    - Calculate and show the *speedup* (or *slowdown*) of your GPU kernel for each $N$ vs *numpy.dot()*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\n",
    "    - For each $N$ vs, calculate and show the *speedup* of your GPU kernel using *dtype=float32* vs *dtype=float64*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\"\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "h_A = np.random.random((N, N)).astype(np.<float-type>)\n",
    "h_B = np.random.random((N, N)).astype(np.<float-type>)\n",
    "```    \n",
    "<br>\n",
    "\n",
    "3. For the device memory:\n",
    "    - Create **d_A** and **d_B** by copying **h_A** and **h_B** to the GPU, and be sure to time the copies\n",
    "    - Create **d_C** as device-array that is allocated on the GPU (device) only, and not on the host (**Do Not Copy**)\n",
    "    \n",
    "<br>\n",
    "\n",
    "4. After the GPU matrix-matrix multiplication kernel finishes, **copy** the the *device-array* **d_C** to the *host-array* **h_C**, and be sure to time this copy.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the speedup or slowdowns vs numpy as well as float32 vs float64. Remember, that your runtime for the GPU kernel include time to compile the kernel (not much you can do to control this). Futhermore, becasue you have to copy data to and off of the GPU, these copy times should be included in the \"total-gpu-kernel runtime.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02f6c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function decorator for timing other functions\n",
    "\n",
    "from functools import wraps\n",
    "from time import time, sleep\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def mytimer(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        t_start = time() # Students look here\n",
    "        result = func(*args, **kwargs)\n",
    "        t_end = time() # Students also look here. This is how you can time things inside functions/classes\n",
    "        print(f'Function: \\'{func.__name__}({kwargs})\\' executed in {(t_end-t_start):4.3f}s\\n')\n",
    "        return result\n",
    "    return wrap\n",
    "    \n",
    "\n",
    "# Example of how to use. NOTE the \"@mytimer\" stated just above the function definition\n",
    "@mytimer\n",
    "def example_sum_timer_wrap(N):\n",
    "    \"\"\" Sum the squares of the intergers, i, contained in [1-N] \"\"\"\n",
    "    return np.sum(np.arange(1,N+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40b38051",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest=32\n",
    "N1=5120\n",
    "N2=10240\n",
    "N3=20480\n",
    "\n",
    "#I individually changed size of N to look at the role that matrix size plays.\n",
    "#I wasn't sure/too lazy to find an efficient way to loop :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a016b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_A = np.random.random((N1, N1)).astype(np.float32)\n",
    "#h_B = np.random.random((N1, N1)).astype(np.float32)\n",
    "h_A = np.random.random((N1, N1))\n",
    "h_B = np.random.random((N1, N1))\n",
    "\n",
    "h_A32 = h_A.astype(np.float32)\n",
    "h_B32 = h_A.astype(np.float32)\n",
    "\n",
    "h_A64 = h_A.astype(np.float64)\n",
    "h_B64 = h_A.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99735477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10200662231445312\n",
      "[[1265.58932836 1268.49147887 1276.416139   ... 1278.68302937\n",
      "  1249.71170104 1294.96838136]\n",
      " [1274.93335371 1258.11072075 1269.63152314 ... 1269.13949979\n",
      "  1253.3997985  1295.87762923]\n",
      " [1296.0086385  1281.10968116 1282.40763949 ... 1285.24985428\n",
      "  1269.15932237 1298.0536378 ]\n",
      " ...\n",
      " [1294.25286303 1280.99521044 1287.78793514 ... 1284.76990299\n",
      "  1268.47559971 1317.8841791 ]\n",
      " [1299.75554432 1307.66221258 1304.37306205 ... 1306.63218167\n",
      "  1279.94072433 1330.59064604]\n",
      " [1293.12331209 1275.12425777 1286.82023674 ... 1270.10158207\n",
      "  1259.01523927 1303.02700646]]\n",
      "3.33952099609375\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "#Initialize Total Event\n",
    "start_event = cuda.event()\n",
    "end_event = cuda.event()\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_event.record()\n",
    "\n",
    "\n",
    "# CUDA kernel\n",
    "numba.cuda.event(timing=True)\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "#A = numpy.full((24, 12), 3, numpy.float) # matrix containing all 3's\n",
    "#B = numpy.full((12, 22), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "\n",
    "#Initialize Copy Event\n",
    "start_copyevent = cuda.event()\n",
    "end_copyevent = cuda.event()\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_copyevent.record()\n",
    "\n",
    "# Copy the arrays to the device -- the equivalent of d_A and d_B\n",
    "A_global_mem = cuda.to_device(h_A32)\n",
    "B_global_mem = cuda.to_device(h_B32)\n",
    "\n",
    "# Record the end event\n",
    "end_copyevent.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_copyevent.synchronize()\n",
    "\n",
    "# Compute the Copy elapsed time\n",
    "elapsed_copytime = cuda.event_elapsed_time(start_copyevent, end_copyevent)\n",
    "print(elapsed_copytime/1000)\n",
    "\n",
    "\n",
    "# Allocate memory on the device for the result -- the equivalent of d_C\n",
    "C_global_mem = cuda.device_array((h_A.shape[0],h_B.shape[1]))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(h_A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(h_B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host -- the equivalent of h_C\n",
    "C32 = C_global_mem.copy_to_host()\n",
    "\n",
    "\n",
    "print(C32)\n",
    "\n",
    "# Record the end event\n",
    "end_event.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_event.synchronize()\n",
    "\n",
    "# Compute the total elapsed time\n",
    "elapsed_time = cuda.event_elapsed_time(start_event, end_event)\n",
    "print(elapsed_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2440c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09427375793457031\n",
      "[[1265.589327   1268.49147801 1276.41613835 ... 1278.68302876\n",
      "  1249.71170082 1294.96837963]\n",
      " [1274.93335264 1258.11072159 1269.63152412 ... 1269.13949882\n",
      "  1253.39979946 1295.87762961]\n",
      " [1296.00863823 1281.10968211 1282.4076388  ... 1285.24985331\n",
      "  1269.15932145 1298.05363662]\n",
      " ...\n",
      " [1294.25286023 1280.99521188 1287.78793433 ... 1284.7699011\n",
      "  1268.47559898 1317.88417874]\n",
      " [1299.755544   1307.66221104 1304.37306188 ... 1306.63217996\n",
      "  1279.94072431 1330.59064521]\n",
      " [1293.12331176 1275.12425898 1286.82023578 ... 1270.10157992\n",
      "  1259.01524025 1303.02700638]]\n",
      "5.128408203125\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "#Initialize Total Event\n",
    "start_event = cuda.event()\n",
    "end_event = cuda.event()\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_event.record()\n",
    "\n",
    "\n",
    "# CUDA kernel\n",
    "numba.cuda.event(timing=True)\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "#A = numpy.full((24, 12), 3, numpy.float) # matrix containing all 3's\n",
    "#B = numpy.full((12, 22), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "#Initialize Copy Event\n",
    "start_copyevent = cuda.event()\n",
    "end_copyevent = cuda.event()\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_copyevent.record()\n",
    "\n",
    "# Copy the arrays to the device -- the equivalent of d_A and d_B\n",
    "A_global_mem = cuda.to_device(h_A64)\n",
    "B_global_mem = cuda.to_device(h_B64)\n",
    "\n",
    "# Record the end event\n",
    "end_copyevent.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_copyevent.synchronize()\n",
    "\n",
    "# Compute the Copy elapsed time\n",
    "elapsed_copytime = cuda.event_elapsed_time(start_copyevent, end_copyevent)\n",
    "print(elapsed_copytime/1000)\n",
    "\n",
    "\n",
    "# Allocate memory on the device for the result -- the equivalent of d_C\n",
    "C_global_mem = cuda.device_array((h_A.shape[0],h_B.shape[1]))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(h_A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(h_B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host -- the equivalent of h_C\n",
    "C64 = C_global_mem.copy_to_host()\n",
    "\n",
    "\n",
    "print(C64)\n",
    "\n",
    "# Record the end event\n",
    "end_event.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_event.synchronize()\n",
    "\n",
    "# Compute the total elapsed time\n",
    "elapsed_time = cuda.event_elapsed_time(start_event, end_event)\n",
    "print(elapsed_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ab37426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'npcomparison({})' executed in 0.575s\n",
      "\n",
      "[[1265.5894 1268.4912 1276.4161 ... 1278.6832 1249.7117 1294.9683]\n",
      " [1274.9335 1258.1107 1269.6315 ... 1269.1395 1253.3999 1295.8777]\n",
      " [1296.0088 1281.1096 1282.4076 ... 1285.2499 1269.1593 1298.0537]\n",
      " ...\n",
      " [1294.2529 1280.9951 1287.7881 ... 1284.7701 1268.4755 1317.884 ]\n",
      " [1299.7557 1307.6624 1304.3733 ... 1306.6322 1279.9406 1330.5906]\n",
      " [1293.1234 1275.1243 1286.8202 ... 1270.1017 1259.0154 1303.027 ]]\n"
     ]
    }
   ],
   "source": [
    "#Comparing against numpy.dot\n",
    "@mytimer\n",
    "def npcomparison(A,B):\n",
    "    C=np.dot(A,B)\n",
    "    return C\n",
    "C1=npcomparison(h_A32,h_B32)\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55ff7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.71126628e-05 -2.67931962e-04 -1.30108447e-06 ...  1.98164433e-04\n",
      "  -3.11157910e-05 -1.19640388e-04]\n",
      " [ 1.17973380e-04 -2.97489896e-06 -5.34141241e-05 ...  2.65808103e-05\n",
      "   1.03847340e-04  5.63160138e-05]\n",
      " [ 1.50564225e-04 -6.20173669e-05 -4.67116840e-05 ...  2.36518172e-05\n",
      "  -2.06082641e-05  7.31368759e-05]\n",
      " ...\n",
      " [ 6.66563028e-05 -9.32480798e-05  1.50794437e-04 ...  2.38613055e-04\n",
      "  -1.35846298e-04 -1.45897433e-04]\n",
      " [ 1.92986185e-04  1.40938556e-04  2.28970177e-04 ...  2.04776763e-05\n",
      "  -1.72568635e-04 -6.98667509e-05]\n",
      " [ 1.00993620e-04  9.81204721e-06 -4.63147221e-05 ...  1.02495454e-04\n",
      "   1.41592253e-04 -2.89198833e-05]]\n",
      "[[ 2.84702196e-05 -2.67073501e-04 -6.54315272e-07 ...  1.98783464e-04\n",
      "  -3.08978370e-05 -1.17911885e-04]\n",
      " [ 1.19038325e-04 -3.81651762e-06 -5.43970568e-05 ...  2.75463340e-05\n",
      "   1.02885172e-04  5.59336117e-05]\n",
      " [ 1.50828404e-04 -6.29690630e-05 -4.60249139e-05 ...  2.46187778e-05\n",
      "  -1.96879394e-05  7.43182436e-05]\n",
      " ...\n",
      " [ 6.94617954e-05 -9.46883645e-05  1.51612244e-04 ...  2.40501331e-04\n",
      "  -1.35115647e-04 -1.45538537e-04]\n",
      " [ 1.93307972e-04  1.42472045e-04  2.29131520e-04 ...  2.21869639e-05\n",
      "  -1.72555868e-04 -6.90345428e-05]\n",
      " [ 1.01320967e-04  8.60245314e-06 -4.53495352e-05 ...  1.04654361e-04\n",
      "   1.40605814e-04 -2.88378894e-05]]\n"
     ]
    }
   ],
   "source": [
    "Cdiff32=C1-C32\n",
    "Cdiff64=C1-C64\n",
    "print(Cdiff32)\n",
    "print(Cdiff64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a48f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05046992111206055\n",
      "[[1265.58932836 1268.49147887 1276.416139   ... 1278.68302937\n",
      "  1249.71170104 1294.96838136]\n",
      " [1274.93335371 1258.11072075 1269.63152314 ... 1269.13949979\n",
      "  1253.3997985  1295.87762923]\n",
      " [1296.0086385  1281.10968116 1282.40763949 ... 1285.24985428\n",
      "  1269.15932237 1298.0536378 ]\n",
      " ...\n",
      " [1294.25286303 1280.99521044 1287.78793514 ... 1284.76990299\n",
      "  1268.47559971 1317.8841791 ]\n",
      " [1299.75554432 1307.66221258 1304.37306205 ... 1306.63218167\n",
      "  1279.94072433 1330.59064604]\n",
      " [1293.12331209 1275.12425777 1286.82023674 ... 1270.10158207\n",
      "  1259.01523927 1303.02700646]]\n",
      "3.042510986328125\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda, float32\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "start_event = cuda.event()\n",
    "end_event = cuda.event()\n",
    "\n",
    "# Record the start event\n",
    "start_event.record()\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "#A = numpy.full((TPB*2, TPB*3), 3, numpy.float32) # [32 x 48] matrix containing all 3's\n",
    "#B = numpy.full((TPB*3, TPB*1), 4, numpy.float32) # [48 x 16] matrix containing all 4's\n",
    "\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_event.record()\n",
    "\n",
    "#Initialize Copy Event\n",
    "start_copyevent = cuda.event()\n",
    "end_copyevent = cuda.event()\n",
    "\n",
    "# Record the Start of Copy event\n",
    "start_copyevent.record()\n",
    "\n",
    "A_global_mem = cuda.to_device(h_A32)\n",
    "B_global_mem = cuda.to_device(h_B32)\n",
    "C_global_mem = cuda.device_array((h_A.shape[0],h_B.shape[1]))\n",
    "\n",
    "# Record the end event\n",
    "end_copyevent.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_copyevent.synchronize()\n",
    "\n",
    "# Compute the Copy elapsed time\n",
    "elapsed_copytime = cuda.event_elapsed_time(start_copyevent, end_copyevent)\n",
    "print(elapsed_copytime/1000)\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(math.ceil(h_A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(math.ceil(h_B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res32 = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Record the end event\n",
    "end_event.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_event.synchronize()\n",
    "\n",
    "# Compute the elapsed time\n",
    "elapsed_time = cuda.event_elapsed_time(start_event, end_event)\n",
    "print(elapsed_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c869b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09278489685058594\n",
      "[[1265.58932836 1268.49147887 1276.416139   ... 1278.68302937\n",
      "  1249.71170104 1294.96838136]\n",
      " [1274.93335371 1258.11072075 1269.63152314 ... 1269.13949979\n",
      "  1253.3997985  1295.87762923]\n",
      " [1296.0086385  1281.10968116 1282.40763949 ... 1285.24985428\n",
      "  1269.15932237 1298.0536378 ]\n",
      " ...\n",
      " [1294.25286303 1280.99521044 1287.78793514 ... 1284.76990299\n",
      "  1268.47559971 1317.8841791 ]\n",
      " [1299.75554432 1307.66221258 1304.37306205 ... 1306.63218167\n",
      "  1279.94072433 1330.59064604]\n",
      " [1293.12331209 1275.12425777 1286.82023674 ... 1270.10158207\n",
      "  1259.01523927 1303.02700646]]\n",
      "3.29268310546875\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda, float32\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "start_event = cuda.event()\n",
    "end_event = cuda.event()\n",
    "\n",
    "# Record the start event\n",
    "start_event.record()\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "#A = numpy.full((TPB*2, TPB*3), 3, numpy.float32) # [32 x 48] matrix containing all 3's\n",
    "#B = numpy.full((TPB*3, TPB*1), 4, numpy.float32) # [48 x 16] matrix containing all 4's\n",
    "\n",
    "\n",
    "# Record the Start of Total event\n",
    "start_event.record()\n",
    "\n",
    "#Initialize Copy Event\n",
    "start_copyevent = cuda.event()\n",
    "end_copyevent = cuda.event()\n",
    "\n",
    "# Record the Start of Copy event\n",
    "start_copyevent.record()\n",
    "\n",
    "A_global_mem = cuda.to_device(h_A64)\n",
    "B_global_mem = cuda.to_device(h_B64)\n",
    "C_global_mem = cuda.device_array((h_A.shape[0],h_B.shape[1]))\n",
    "\n",
    "# Record the end event\n",
    "end_copyevent.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_copyevent.synchronize()\n",
    "\n",
    "# Compute the Copy elapsed time\n",
    "elapsed_copytime = cuda.event_elapsed_time(start_copyevent, end_copyevent)\n",
    "print(elapsed_copytime/1000)\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(math.ceil(h_A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(math.ceil(h_B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res64 = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Record the end event\n",
    "end_event.record()\n",
    "\n",
    "# Synchronize to wait for the events to complete\n",
    "end_event.synchronize()\n",
    "\n",
    "# Compute the elapsed time\n",
    "elapsed_time = cuda.event_elapsed_time(start_event, end_event)\n",
    "print(elapsed_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0bc4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'npcomparison({})' executed in 0.550s\n",
      "\n",
      "[[1265.5894 1268.4912 1276.4161 ... 1278.6832 1249.7117 1294.9683]\n",
      " [1274.9335 1258.1107 1269.6315 ... 1269.1395 1253.3999 1295.8777]\n",
      " [1296.0088 1281.1096 1282.4076 ... 1285.2499 1269.1593 1298.0537]\n",
      " ...\n",
      " [1294.2529 1280.9951 1287.7881 ... 1284.7701 1268.4755 1317.884 ]\n",
      " [1299.7557 1307.6624 1304.3733 ... 1306.6322 1279.9406 1330.5906]\n",
      " [1293.1234 1275.1243 1286.8202 ... 1270.1017 1259.0154 1303.027 ]]\n"
     ]
    }
   ],
   "source": [
    "#Comparing against numpy.dot\n",
    "#Comparing against numpy.dot\n",
    "#Comparing against numpy.dot\n",
    "@mytimer\n",
    "def npcomparison(A,B):\n",
    "    C=np.dot(A,B)\n",
    "    return C\n",
    "C2=npcomparison(h_A32,h_B32)\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3cac361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.71126628e-05 -2.67931962e-04 -1.30108447e-06 ...  1.98164433e-04\n",
      "  -3.11157910e-05 -1.19640388e-04]\n",
      " [ 1.17973380e-04 -2.97489896e-06 -5.34141241e-05 ...  2.65808103e-05\n",
      "   1.03847340e-04  5.63160138e-05]\n",
      " [ 1.50564225e-04 -6.20173669e-05 -4.67116840e-05 ...  2.36518172e-05\n",
      "  -2.06082641e-05  7.31368759e-05]\n",
      " ...\n",
      " [ 6.66563028e-05 -9.32480798e-05  1.50794437e-04 ...  2.38613055e-04\n",
      "  -1.35846298e-04 -1.45897433e-04]\n",
      " [ 1.92986185e-04  1.40938556e-04  2.28970177e-04 ...  2.04776763e-05\n",
      "  -1.72568635e-04 -6.98667509e-05]\n",
      " [ 1.00993620e-04  9.81204721e-06 -4.63147221e-05 ...  1.02495454e-04\n",
      "   1.41592253e-04 -2.89198833e-05]]\n",
      "[[ 2.71126628e-05 -2.67931962e-04 -1.30108447e-06 ...  1.98164433e-04\n",
      "  -3.11157910e-05 -1.19640388e-04]\n",
      " [ 1.17973380e-04 -2.97489896e-06 -5.34141241e-05 ...  2.65808103e-05\n",
      "   1.03847340e-04  5.63160138e-05]\n",
      " [ 1.50564225e-04 -6.20173669e-05 -4.67116840e-05 ...  2.36518172e-05\n",
      "  -2.06082641e-05  7.31368759e-05]\n",
      " ...\n",
      " [ 6.66563028e-05 -9.32480798e-05  1.50794437e-04 ...  2.38613055e-04\n",
      "  -1.35846298e-04 -1.45897433e-04]\n",
      " [ 1.92986185e-04  1.40938556e-04  2.28970177e-04 ...  2.04776763e-05\n",
      "  -1.72568635e-04 -6.98667509e-05]\n",
      " [ 1.00993620e-04  9.81204721e-06 -4.63147221e-05 ...  1.02495454e-04\n",
      "   1.41592253e-04 -2.89198833e-05]]\n",
      "[[ 2.71126628e-05 -2.67931962e-04 -1.30108447e-06 ...  1.98164433e-04\n",
      "  -3.11157910e-05 -1.19640388e-04]\n",
      " [ 1.17973380e-04 -2.97489896e-06 -5.34141241e-05 ...  2.65808103e-05\n",
      "   1.03847340e-04  5.63160138e-05]\n",
      " [ 1.50564225e-04 -6.20173669e-05 -4.67116840e-05 ...  2.36518172e-05\n",
      "  -2.06082641e-05  7.31368759e-05]\n",
      " ...\n",
      " [ 6.66563028e-05 -9.32480798e-05  1.50794437e-04 ...  2.38613055e-04\n",
      "  -1.35846298e-04 -1.45897433e-04]\n",
      " [ 1.92986185e-04  1.40938556e-04  2.28970177e-04 ...  2.04776763e-05\n",
      "  -1.72568635e-04 -6.98667509e-05]\n",
      " [ 1.00993620e-04  9.81204721e-06 -4.63147221e-05 ...  1.02495454e-04\n",
      "   1.41592253e-04 -2.89198833e-05]]\n"
     ]
    }
   ],
   "source": [
    "C2diff=C2-res\n",
    "print(C2diff)\n",
    "\n",
    "C2diff32=C2-res32\n",
    "C2diff64=C2-res64\n",
    "print(C2diff32)\n",
    "print(C2diff64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f35012",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Response\n",
    "\n",
    "Both the numpy and cuda uses vastly increase the speedup for the matrix multiplacation compared to the naive CPU matrix version. For all of the matrix cases except for the largest one, numpy was faster than the GPUs. As the matrix sizes got bigger, it became more and more favorable to use the GPU's. The GPUs were twice as slow for float64 than float32. This effect isn't as stark as the change in speeds for increasing matrix sizes. The biggest time holdup was just waiting for the data to be copied to the GPU and then for the information to come back. Also, it was only worth using the GPU when they were all linked up (second copied NYU code) -- so knowing how to configure them. For the most part, it's easiest for the programmer to live in numpy world. Apologies for not looping throught all of the N's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd6245",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: CuPy \n",
    "\n",
    "For this exercise, we will repeat what we did in *Exercise 2*. However, we will use *CuPy* functions, which are similar to *Numpy* funcstions with some added functions for copying data to-the-device-from-the-host and to-the-host-from-the-device. By using CuPy, we can depend on code that has been optimized for the GPU by NVIDIA, and instead of tyring to optimize our matrix-matrix multiplication kernels, we can use a built-in function to calculate the multiplication instead (i.e. [cupy.dot()](https://docs.cupy.dev/en/stable/reference/generated/cupy.dot.html#cupy.dot)).\n",
    "\n",
    "**Tasks for this exercise:**\n",
    "* Same as those listed in *Exercise 2*, but compare *cupy.dot()* to *numpy.dot()*.\n",
    "* Also, reuse the host-arrays, *h_A* and *h_B* above. You will need to call the appropriate *CuPy* fuctions to copy these arrays to the GPU and to copy the result back to the host. You will **not** need to declare the deive-C array before calling *cupy.dot()* because the function will do it for you (like numpy does).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d6103d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "def test_cupy_dot32():\n",
    "    h_A = np.random.random((N1,N1)).astype('float32') # generate a random matrix on the CPU\n",
    "    h_B = np.random.random((N1,N1)).astype('float32')\n",
    "    d_A = cp.asarray(h_A)\n",
    "    d_B = cp.asarray(h_B)\n",
    "    start_event = cp.cuda.Event()\n",
    "    end_event = cp.cuda.Event()\n",
    "    start_event.record()\n",
    "    #CuPy\n",
    "    d_C = cp.dot(d_A, d_B)\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    gpu_time = cp.cuda.get_elapsed_time(start_event, end_event)\n",
    "    h_C = d_C.get()\n",
    "    h_ref = np.dot(h_A, h_B)\n",
    "    error = np.linalg.norm(h_ref - h_C) / np.linalg.norm(h_ref)\n",
    "    print(gpu_time/100)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a848294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "def test_cupy_dot64():\n",
    "    h_A = np.random.random((N1,N1)).astype('float64') # generate a random matrix on the CPU\n",
    "    h_B = np.random.random((N1,N1)).astype('float64')\n",
    "    d_A = cp.asarray(h_A)\n",
    "    d_B = cp.asarray(h_B)\n",
    "    start_event = cp.cuda.Event()\n",
    "    end_event = cp.cuda.Event()\n",
    "    start_event.record()\n",
    "    #CuPy\n",
    "    d_C = cp.dot(d_A, d_B)\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    gpu_time = cp.cuda.get_elapsed_time(start_event, end_event)\n",
    "    h_C = d_C.get()\n",
    "    h_ref = np.dot(h_A, h_B)\n",
    "    error = np.linalg.norm(h_ref - h_C) / np.linalg.norm(h_ref)\n",
    "    print(gpu_time/100)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95ef692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run and error for d32\n",
      "0.9117759704589844\n",
      "1.1077095e-06\n",
      "time to run and error for d64\n",
      "11.995714111328125\n",
      "2.064249053257417e-15\n"
     ]
    }
   ],
   "source": [
    "print('time to run and error for d32')\n",
    "test_cupy_dot32()\n",
    "print('time to run and error for d64')\n",
    "test_cupy_dot64()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e1dfb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise $\\mathbf{\\pi}$: CuPy Interoperability\n",
    "\n",
    "Numba and CuPy device arrays (GPU arrays) can be accept each other's arrays. See [Interoperability](https://docs.cupy.dev/en/stable/user_guide/interoperability.html).\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Use the **device** arrays, **d_A** and **d_B**, that were created in *Exercise 2* to calculate the matrix-matrix multiplcation using *cupy.dot()*.\n",
    "* Verify that you get the same results as you did in *Exercise 3*.\n",
    "* You will need to \"wrap\" the device arrays before passing them to *cupy.dot()*. Read the *Interoperability* documentation linked above.\n",
    "    - Time how long it takes (runtime) to \"wrap\" these arrays.\n",
    "    - Compare this runtime to the runtime it took to create the device arrays in *Exercise 3*.\n",
    "    - Provide a quick comment your thoughts on the runtime differences compared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7dbb6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0658392160139756e-15\n",
      "Time to wrap device arrays: 0.00018 seconds\n",
      "Time to perform matrix multiplication: 0.00018 seconds\n"
     ]
    }
   ],
   "source": [
    "def test_matmul(d_A, d_B):\n",
    "    # Wrap the device arrays\n",
    "    wrapped_d_A = cp.asarray(d_A)\n",
    "    wrapped_d_B = cp.asarray(d_B)\n",
    "\n",
    "    # Calculate matrix multiplication using cupy.dot()\n",
    "    start_wrap = time.time()\n",
    "    result = cp.dot(wrapped_d_A, wrapped_d_B)\n",
    "    end_wrap = time.time()\n",
    "\n",
    "    # copy result back to host memory\n",
    "    h_result = result.get()\n",
    "\n",
    "    # compare with reference numpy result\n",
    "    h_ref = np.dot(d_A.copy_to_host(), d_B.copy_to_host())\n",
    "    error = np.linalg.norm(h_ref - h_result) / np.linalg.norm(h_ref)\n",
    "\n",
    "    # Print results\n",
    "    # print(f\"Cupy dot result:\\n{h_result}\\n\")\n",
    "    # print(f\"Reference numpy dot result:\\n{h_ref}\\n\")\n",
    "    print(error)\n",
    "    print(f\"Time to wrap device arrays: {end_wrap - start_wrap:.5f} seconds\")\n",
    "    print(f\"Time to perform matrix multiplication: {end_wrap - start_wrap:.5f} seconds\")\n",
    "    \n",
    "def test_wrap():\n",
    "    d_A = cuda.to_device(h_A)\n",
    "    d_B = cuda.to_device(h_B)\n",
    "\n",
    "    #  test_matmul function to wrap device arrays and perform matrix multiplication\n",
    "    test_matmul(d_A, d_B)\n",
    "            \n",
    "test_wrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce8a05",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise $\\mathbf{\\pi}$: Response\n",
    "\n",
    "The runtimes are lowest for the test_wrap scenario. With excercise 3, we would have to pay the cost of wrapping for each iteration we ran, which adds up. For this case, we only have to pay the cost once because we wrap over all of the runs. It takes a lot of time for the information to copy over between machines. We want to minimize the copying and reading as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657abb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
